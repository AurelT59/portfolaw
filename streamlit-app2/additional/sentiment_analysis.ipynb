{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9177e-b717-48c0-beb5-5ff8033a4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "reglaw_social_sentiment_temporal_v3_llmdate_cap50.py\n",
    "\n",
    "Version TEMPORELLE + LLM pour la date + CAP PAR ENTREPRISE\n",
    "\n",
    "Objectifs ajoutés par rapport à v2 :\n",
    "- limiter le nombre de posts/commentaires/articles CONSERVÉS par entreprise (ex: 50)\n",
    "- scorer les posts par pertinence et ne garder que les plus liés à la directive\n",
    "- recalculer les agrégats de sentiment APRÈS filtrage\n",
    "- éviter de continuer à scraper si on a déjà assez de bons posts pour une entreprise\n",
    "\n",
    "ENV nouveaux :\n",
    "- MAX_POSTS_PER_COMPANY (default=50)\n",
    "- EARLY_STOP_FACTOR (default=1.5) → on arrête de requêter quand on a 1.5 × max dans le buffer\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import unicodedata\n",
    "from hashlib import md5\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import boto3\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG GLOBALE\n",
    "# =========================================================\n",
    "\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-west-2\")\n",
    "DIRECTIVES_DIR = os.getenv(\"DIRECTIVES_DIR\", \"/home/sagemaker-user/shared/directives\")\n",
    "SP500_PATH = os.getenv(\"SP500_PATH\", \"/home/sagemaker-user/shared/2025-08-15_composition_sp500.csv\")\n",
    "OUTPUT_BASE = os.getenv(\"OUTPUT_BASE\", \"/home/sagemaker-user/shared/sentiment_analysis\")\n",
    "\n",
    "MAX_COMPANIES = int(os.getenv(\"MAX_COMPANIES\", \"50\"))     # 0 = toutes\n",
    "POSTS_PER_QUERY = int(os.getenv(\"POSTS_PER_QUERY\", \"30\"))\n",
    "\n",
    "MAX_POSTS_PER_COMPANY = int(os.getenv(\"MAX_POSTS_PER_COMPANY\", \"30\"))\n",
    "EARLY_STOP_FACTOR = float(os.getenv(\"EARLY_STOP_FACTOR\", \"1.5\"))  # on arrête de requêter quand buffer >= factor * max\n",
    "\n",
    "# mode temporel : par défaut on veut la date de la loi\n",
    "TEMPORAL_MODE = os.getenv(\"TEMPORAL_MODE\", \"law_date\").lower()\n",
    "\n",
    "# fenêtre autour de la loi\n",
    "IMPLEMENTATION_BEFORE_DAYS = int(os.getenv(\"IMPLEMENTATION_BEFORE_DAYS\", \"7\"))\n",
    "IMPLEMENTATION_AFTER_DAYS = int(os.getenv(\"IMPLEMENTATION_AFTER_DAYS\", \"14\"))\n",
    "\n",
    "# fallback \"continu\" si rien\n",
    "DAYS_BACK = int(os.getenv(\"DAYS_BACK\", \"30\"))\n",
    "PUSHSHIFT_RECENT_AFTER = f\"{DAYS_BACK}d\"\n",
    "\n",
    "ENABLE_REDDIT = os.getenv(\"ENABLE_REDDIT\", \"1\") == \"1\"\n",
    "ENABLE_HN = os.getenv(\"ENABLE_HN\", \"1\") == \"1\"\n",
    "ENABLE_MASTODON = os.getenv(\"ENABLE_MASTODON\", \"1\") == \"1\"\n",
    "\n",
    "# poids\n",
    "WEIGHT_STRONG = float(os.getenv(\"WEIGHT_STRONG\", \"1.0\"))\n",
    "WEIGHT_POLICY = float(os.getenv(\"WEIGHT_POLICY\", \"0.7\"))\n",
    "WEIGHT_WEAK = float(os.getenv(\"WEIGHT_WEAK\", \"0.35\"))\n",
    "WEIGHT_VERY_WEAK = float(os.getenv(\"WEIGHT_VERY_WEAK\", \"0.1\"))\n",
    "\n",
    "# LLM\n",
    "USE_LLM_KEYWORDS = os.getenv(\"USE_LLM_KEYWORDS\", \"1\") == \"1\"\n",
    "USE_LLM_DATES = os.getenv(\"USE_LLM_DATES\", \"1\") == \"1\"\n",
    "\n",
    "BEDROCK_MODEL_ID = os.getenv(\"BEDROCK_MODEL_ID\", \"anthropic.claude-3-5-haiku-20241022-v1:0\")\n",
    "BEDROCK_MODEL_ID_DATES = os.getenv(\"BEDROCK_MODEL_ID_DATES\", BEDROCK_MODEL_ID)\n",
    "\n",
    "# AWS clients\n",
    "comprehend = boto3.client(\"comprehend\", region_name=AWS_REGION)\n",
    "translate = boto3.client(\"translate\", region_name=AWS_REGION) if os.getenv(\"USE_TRANSLATE\", \"1\") == \"1\" else None\n",
    "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n",
    "\n",
    "# Pushshift endpoints\n",
    "PUSHSHIFT_SUBMISSIONS = [\n",
    "    \"https://api.pushshift.io/reddit/search/submission\",\n",
    "    \"https://api.pullpush.io/reddit/search/submission\",\n",
    "]\n",
    "PUSHSHIFT_COMMENTS = [\n",
    "    \"https://api.pushshift.io/reddit/search/comment\",\n",
    "    \"https://api.pullpush.io/reddit/search/comment\",\n",
    "]\n",
    "\n",
    "# Mastodon\n",
    "MASTODON_INSTANCES = [\n",
    "    \"mastodon.social\",\n",
    "    \"mstdn.social\",\n",
    "]\n",
    "\n",
    "# petits mois pour fallback regex\n",
    "MONTHS_EN = {\n",
    "    \"january\": 1, \"february\": 2, \"march\": 3, \"april\": 4,\n",
    "    \"may\": 5, \"june\": 6, \"july\": 7, \"august\": 8,\n",
    "    \"september\": 9, \"october\": 10, \"november\": 11, \"december\": 12\n",
    "}\n",
    "MONTHS_FR = {\n",
    "    \"janvier\": 1, \"février\": 2, \"fevrier\": 2, \"mars\": 3, \"avril\": 4,\n",
    "    \"mai\": 5, \"juin\": 6, \"juillet\": 7, \"août\": 8, \"aout\": 8,\n",
    "    \"septembre\": 9, \"octobre\": 10, \"novembre\": 11, \"décembre\": 12, \"decembre\": 12\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# UTILS GÉNÉRAUX\n",
    "# =========================================================\n",
    "\n",
    "def slugify(value: str) -> str:\n",
    "    value = unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    value = re.sub(r\"[^\\w\\s-]\", \"\", value).strip().lower()\n",
    "    value = re.sub(r\"[-\\s]+\", \"-\", value)\n",
    "    return value\n",
    "\n",
    "\n",
    "def list_directives(path: str) -> List[str]:\n",
    "    files = sorted(glob.glob(os.path.join(path, \"*\")))\n",
    "    out = []\n",
    "    for f in files:\n",
    "        base = os.path.basename(f).lower()\n",
    "        if base.startswith(\"readme\") or base.endswith(\".md\"):\n",
    "            continue\n",
    "        out.append(f)\n",
    "    return out\n",
    "\n",
    "\n",
    "def read_directive_text(path: str) -> str:\n",
    "    with io.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw = f.read()\n",
    "    if path.lower().endswith(\".xml\"):\n",
    "        soup = BeautifulSoup(raw, \"xml\")\n",
    "    else:\n",
    "        soup = BeautifulSoup(raw, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"meta\", \"link\"]):\n",
    "        tag.decompose()\n",
    "    return soup.get_text(\" \", strip=True)\n",
    "\n",
    "\n",
    "def guess_title_from_filename(path: str) -> str:\n",
    "    base = os.path.basename(path)\n",
    "    base = re.sub(r\"\\.(html?|xml)$\", \"\", base, flags=re.IGNORECASE)\n",
    "    base = re.sub(r\"^\\d+\\.\", \"\", base).strip()\n",
    "    return base\n",
    "\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    try:\n",
    "        resp = comprehend.detect_dominant_language(Text=text[:4000])\n",
    "        langs = resp.get(\"Languages\", [])\n",
    "        if not langs:\n",
    "            return \"unknown\"\n",
    "        langs = sorted(langs, key=lambda x: x[\"Score\"], reverse=True)\n",
    "        return langs[0][\"LanguageCode\"]\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def translate_to_en(text: str, src: str) -> str:\n",
    "    if not translate or src == \"en\":\n",
    "        return text\n",
    "    try:\n",
    "        resp = translate.translate_text(Text=text[:4500], SourceLanguageCode=src, TargetLanguageCode=\"en\")\n",
    "        return resp.get(\"TranslatedText\", text)\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "# =========================================================\n",
    "# LLM : EXTRACTION DES DATES\n",
    "# =========================================================\n",
    "\n",
    "LLM_DATE_PROMPT_BASE = (\n",
    "    \"You are an assistant that extracts DATES from an EU/US/CA regulation or directive.\\n\"\n",
    "    \"You MUST answer in STRICT JSON, no explanation, no markdown.\\n\\n\"\n",
    "    \"You will receive:\\n\"\n",
    "    \"- the official title\\n\"\n",
    "    \"- the English version (or translation) of the directive\\n\"\n",
    "    \"Your task is to find the dates that matter for when the obligations start.\\n\\n\"\n",
    "    \"Return EXACTLY this JSON:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"application_date\": \"YYYY-MM-DD or null\",\\n'\n",
    "    '  \"entry_into_force_date\": \"YYYY-MM-DD or null\",\\n'\n",
    "    '  \"transposition_deadline\": \"YYYY-MM-DD or null\",\\n'\n",
    "    '  \"notes\": \"short note if needed\"\\n'\n",
    "    \"}\\n\\n\"\n",
    "    \"Definitions:\\n\"\n",
    "    \"- application_date: when the rules start to apply to companies / market (e.g. \\\"shall apply from 28 May 2022\\\")\\n\"\n",
    "    \"- entry_into_force_date: when the act formally enters into force (often 20 days after publication)\\n\"\n",
    "    \"- transposition_deadline: when Member States must transpose the directive\\n\\n\"\n",
    "    \"If you don't find a date for a field, put null.\\n\"\n",
    ")\n",
    "\n",
    "def _try_parse_iso_date(s: str) -> Optional[datetime.date]:\n",
    "    if not s or s.lower() == \"null\":\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.date.fromisoformat(s.strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _extract_json_from_text(text: str) -> Optional[dict]:\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(m.group(0))\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def call_bedrock_for_dates(title: str, directive_en: str) -> Optional[dict]:\n",
    "    if not USE_LLM_DATES:\n",
    "        return None\n",
    "\n",
    "    prompt = (\n",
    "        LLM_DATE_PROMPT_BASE\n",
    "        + f\"Title: {title}\\n\"\n",
    "        + \"Directive (english or translated) excerpt:\\n\"\n",
    "        + directive_en[:5000]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        resp = bedrock_rt.invoke_model(\n",
    "            modelId=BEDROCK_MODEL_ID_DATES,\n",
    "            body=json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 600,\n",
    "                \"temperature\": 0.0,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            })\n",
    "        )\n",
    "        raw = resp[\"body\"].read().decode(\"utf-8\")\n",
    "\n",
    "        try:\n",
    "            obj = json.loads(raw)\n",
    "            if isinstance(obj, dict) and \"content\" in obj:\n",
    "                txt = obj[\"content\"][0][\"text\"]\n",
    "                extracted = _extract_json_from_text(txt)\n",
    "                return extracted\n",
    "            return obj\n",
    "        except json.JSONDecodeError:\n",
    "            extracted = _extract_json_from_text(raw)\n",
    "            return extracted\n",
    "    except Exception as e:\n",
    "        print(f\"[LLM-DATES] error calling Bedrock: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================================================\n",
    "# FALLBACK REGEX\n",
    "# =========================================================\n",
    "\n",
    "def _parse_date_en(day: str, month: str, year: str) -> Optional[datetime.date]:\n",
    "    m = MONTHS_EN.get(month.lower())\n",
    "    if not m:\n",
    "        return None\n",
    "    return datetime.date(int(year), m, int(day))\n",
    "\n",
    "def _parse_date_fr(day: str, month: str, year: str) -> Optional[datetime.date]:\n",
    "    m = MONTHS_FR.get(month.lower())\n",
    "    if not m:\n",
    "        return None\n",
    "    return datetime.date(int(year), m, int(day))\n",
    "\n",
    "def guess_implementation_date_regex(text: str) -> Optional[datetime.date]:\n",
    "    m = re.search(r\"(apply|applies|shall apply)\\s+(from|as of)\\s+(\\d{1,2})\\s+([A-Za-z]+)\\s+(\\d{4})\", text, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        d, month, y = m.group(3), m.group(4), m.group(5)\n",
    "        dt = _parse_date_en(d, month, y)\n",
    "        if dt:\n",
    "            return dt\n",
    "\n",
    "    m = re.search(r\"(s'applique|est applicable|sera applicable)\\s+(?:à partir du|à compter du|à compter de|depuis le)\\s+(\\d{1,2})\\s+([A-Za-zéûôîàù]+)\\s+(\\d{4})\", text, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        d, month, y = m.group(2), m.group(3), m.group(4)\n",
    "        dt = _parse_date_fr(d, month, y)\n",
    "        if dt:\n",
    "            return dt\n",
    "\n",
    "    m = re.search(r\"(enter into force|enters into force|entre en vigueur)\\s+(on|le)\\s+(\\d{1,2})\\s+([A-Za-zéûôîàù]+)\\s+(\\d{4})\", text, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        d, month, y = m.group(3), m.group(4), m.group(5)\n",
    "        dt_en = _parse_date_en(d, month, y)\n",
    "        if dt_en:\n",
    "            return dt_en\n",
    "        dt_fr = _parse_date_fr(d, month, y)\n",
    "        if dt_fr:\n",
    "            return dt_fr\n",
    "\n",
    "    return None\n",
    "\n",
    "# =========================================================\n",
    "# CONSTRUCTION FENÊTRE\n",
    "# =========================================================\n",
    "\n",
    "def compute_temporal_window_via_llm(directive_title: str,\n",
    "                                    directive_text: str,\n",
    "                                    directive_lang: str) -> Tuple[object, Optional[int], Optional[int], Optional[int], Optional[int], str]:\n",
    "    if TEMPORAL_MODE != \"law_date\":\n",
    "        return (PUSHSHIFT_RECENT_AFTER, None, None, None, \"recent\")\n",
    "\n",
    "    directive_en = translate_to_en(directive_text, directive_lang)\n",
    "\n",
    "    llm_dates = call_bedrock_for_dates(directive_title, directive_en)\n",
    "\n",
    "    chosen_date: Optional[datetime.date] = None\n",
    "\n",
    "    if llm_dates:\n",
    "        app = _try_parse_iso_date(llm_dates.get(\"application_date\"))\n",
    "        eif = _try_parse_iso_date(llm_dates.get(\"entry_into_force_date\"))\n",
    "        trans = _try_parse_iso_date(llm_dates.get(\"transposition_deadline\"))\n",
    "        chosen_date = app or eif or trans\n",
    "\n",
    "    if not chosen_date:\n",
    "        chosen_date = guess_implementation_date_regex(directive_text)\n",
    "\n",
    "    if not chosen_date:\n",
    "        return (PUSHSHIFT_RECENT_AFTER, None, None, None, \"recent\")\n",
    "\n",
    "    start_dt = chosen_date - datetime.timedelta(days=IMPLEMENTATION_BEFORE_DAYS)\n",
    "    end_dt = chosen_date + datetime.timedelta(days=IMPLEMENTATION_AFTER_DAYS)\n",
    "\n",
    "    start_ts = int(datetime.datetime.combine(start_dt, datetime.time.min).timestamp())\n",
    "    end_ts = int(datetime.datetime.combine(end_dt, datetime.time.max).timestamp())\n",
    "\n",
    "    return (start_ts, end_ts, start_ts, end_ts, \"law_date\")\n",
    "\n",
    "# =========================================================\n",
    "# LLM KEYWORDING\n",
    "# =========================================================\n",
    "\n",
    "LLM_PROMPT_TEMPLATE = \"\"\"You are an assistant that extracts search keywords from an EU/US/CA regulation text.\n",
    "\n",
    "You will receive:\n",
    "1) the official title of the directive/regulation\n",
    "2) the English translation of its body (possibly truncated)\n",
    "\n",
    "You must output STRICT JSON, no explanation, no markdown.\n",
    "\n",
    "Rules:\n",
    "- detect official / close names (e.g. \"Directive (EU) 2019/2161\", \"Omnibus Directive\", \"EU consumer modernization directive\")\n",
    "- detect policy / regulatory terms related to consumer protection, sanctions, unfair commercial practices, transparency\n",
    "- detect how people might talk about this on Reddit/Hacker News/Mastodon (short phrases, lowercase ok)\n",
    "- detect obvious noise terms for tech companies (GPU, driver, ARM bid) so that we can downweight them\n",
    "- add short multilingual variants if the original language is French\n",
    "\n",
    "Return JSON with exactly these keys:\n",
    "{{\n",
    "  \"exact_mentions\": [],\n",
    "  \"policy_terms\": [],\n",
    "  \"social_phrases\": [],\n",
    "  \"anti_noise\": [],\n",
    "  \"lang_variants\": []\n",
    "}}\n",
    "\n",
    "If some list is empty, return [].\n",
    "Title: {title}\n",
    "Language: {lang}\n",
    "Directive_english_excerpt:\n",
    "{body}\n",
    "\"\"\"\n",
    "\n",
    "def call_bedrock_for_keywords(title: str, directive_en: str, lang: str) -> Optional[Dict[str, List[str]]]:\n",
    "    if not USE_LLM_KEYWORDS:\n",
    "        return None\n",
    "\n",
    "    prompt = LLM_PROMPT_TEMPLATE.format(\n",
    "        title=title,\n",
    "        body=directive_en[:5000],\n",
    "        lang=lang\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        resp = bedrock_rt.invoke_model(\n",
    "            modelId=BEDROCK_MODEL_ID,\n",
    "            body=json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 1200,\n",
    "                \"temperature\": 0.2,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            })\n",
    "        )\n",
    "        raw = resp[\"body\"].read().decode(\"utf-8\")\n",
    "\n",
    "        try:\n",
    "            direct_json = json.loads(raw)\n",
    "            if isinstance(direct_json, dict) and \"content\" in direct_json:\n",
    "                txt = direct_json[\"content\"][0][\"text\"]\n",
    "                extracted = _extract_json_from_text(txt)\n",
    "                return extracted\n",
    "            return direct_json\n",
    "        except json.JSONDecodeError:\n",
    "            extracted = _extract_json_from_text(raw)\n",
    "            return extracted\n",
    "    except Exception as e:\n",
    "        print(f\"[LLM] error calling Bedrock for keywords: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_dynamic_lexicon(directive_title: str, directive_text: str, directive_lang: str) -> Dict[str, List[str]]:\n",
    "    directive_en = translate_to_en(directive_text, directive_lang)\n",
    "\n",
    "    if USE_LLM_KEYWORDS:\n",
    "        llm_res = call_bedrock_for_keywords(directive_title, directive_en, directive_lang)\n",
    "    else:\n",
    "        llm_res = None\n",
    "\n",
    "    if llm_res:\n",
    "        return {\n",
    "            \"exact_mentions\": [x.strip() for x in llm_res.get(\"exact_mentions\", []) if x.strip()],\n",
    "            \"policy_terms\": [x.strip() for x in llm_res.get(\"policy_terms\", []) if x.strip()],\n",
    "            \"social_phrases\": [x.strip() for x in llm_res.get(\"social_phrases\", []) if x.strip()],\n",
    "            \"anti_noise\": [x.strip() for x in llm_res.get(\"anti_noise\", []) if x.strip()],\n",
    "            \"lang_variants\": [x.strip() for x in llm_res.get(\"lang_variants\", []) if x.strip()],\n",
    "        }\n",
    "\n",
    "    # fallback\n",
    "    print(\"[LLM] fallback to static small lexicon\")\n",
    "    base = [\n",
    "        directive_title,\n",
    "        \"EU consumer law\",\n",
    "        \"EU consumer protection rules\",\n",
    "        \"unfair commercial practices\",\n",
    "        \"EU directive\",\n",
    "        \"EU regulation\",\n",
    "        \"European Commission\",\n",
    "        \"EU sanctions\",\n",
    "    ]\n",
    "    return {\n",
    "        \"exact_mentions\": base,\n",
    "        \"policy_terms\": [\n",
    "            \"consumer protection\", \"consumer rights\", \"price reduction\", \"transparency\", \"online marketplace\"\n",
    "        ],\n",
    "        \"social_phrases\": [\n",
    "            \"eu fines\", \"new eu rules\", \"eu cracked down\", \"compliance with eu\", \"european consumer law\"\n",
    "        ],\n",
    "        \"anti_noise\": [\n",
    "            \"gpu\", \"rtx\", \"driver\", \"arm bid\", \"nvidia driver\", \"4090\"\n",
    "        ],\n",
    "        \"lang_variants\": [\n",
    "            directive_title,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# COMPANIES\n",
    "# =========================================================\n",
    "\n",
    "def load_companies(path: str, limit: int = 0) -> List[Dict[str, str]]:\n",
    "    df = pd.read_csv(path)\n",
    "    name_cols = [c for c in df.columns if \"name\" in c.lower() or \"company\" in c.lower()]\n",
    "    ticker_cols = [c for c in df.columns if \"ticker\" in c.lower() or \"symbol\" in c.lower()]\n",
    "    companies = []\n",
    "    for _, row in df.iterrows():\n",
    "        companies.append({\n",
    "            \"name\": str(row[name_cols[0]]) if name_cols else \"\",\n",
    "            \"ticker\": str(row[ticker_cols[0]]) if ticker_cols else \"\",\n",
    "        })\n",
    "    if limit and limit > 0:\n",
    "        companies = companies[:limit]\n",
    "    return companies\n",
    "\n",
    "# =========================================================\n",
    "# QUERY BUILDER\n",
    "# =========================================================\n",
    "\n",
    "def build_queries_for_company(lex: Dict[str, List[str]], company_name: str, ticker: str) -> List[str]:\n",
    "    q: List[str] = []\n",
    "    for t in lex[\"exact_mentions\"]:\n",
    "        if \" \" in t:\n",
    "            q.append(f'\"{t}\" {company_name}')\n",
    "            if ticker:\n",
    "                q.append(f'\"{t}\" {ticker}')\n",
    "        else:\n",
    "            q.append(f\"{t} {company_name}\")\n",
    "            if ticker:\n",
    "                q.append(f\"{t} {ticker}\")\n",
    "\n",
    "    for p in lex[\"policy_terms\"]:\n",
    "        q.append(f'\"{p}\" {company_name}')\n",
    "        if ticker:\n",
    "            q.append(f'\"{p}\" {ticker}')\n",
    "\n",
    "    for s in lex[\"social_phrases\"]:\n",
    "        q.append(f'\"{s}\" {company_name}')\n",
    "        if ticker:\n",
    "            q.append(f'\"{s}\" {ticker}')\n",
    "\n",
    "    q.append(company_name)\n",
    "    if ticker:\n",
    "        q.append(ticker)\n",
    "\n",
    "    final, seen = [], set()\n",
    "    for x in q:\n",
    "        x = x.strip()\n",
    "        if not x or x in seen:\n",
    "            continue\n",
    "        seen.add(x)\n",
    "        final.append(x)\n",
    "    return final\n",
    "\n",
    "# =========================================================\n",
    "# SENTIMENT\n",
    "# =========================================================\n",
    "\n",
    "def detect_targeted_or_doc_sentiment(text: str, company_name: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        lang_resp = comprehend.detect_dominant_language(Text=text[:4000])\n",
    "        langs = sorted(lang_resp.get(\"Languages\", []), key=lambda x: x[\"Score\"], reverse=True)\n",
    "        lang = langs[0][\"LanguageCode\"] if langs else \"en\"\n",
    "    except Exception:\n",
    "        lang = \"en\"\n",
    "\n",
    "    if lang != \"en\" and translate:\n",
    "        text_en = translate_to_en(text, lang)\n",
    "    else:\n",
    "        text_en = text\n",
    "\n",
    "    try:\n",
    "        resp = comprehend.detect_targeted_sentiment(Text=text_en, LanguageCode=\"en\")\n",
    "        comp_low = company_name.lower()\n",
    "        for ent in resp.get(\"Entities\", []):\n",
    "            if comp_low in ent.get(\"Text\", \"\").lower():\n",
    "                return {\n",
    "                    \"Sentiment\": ent.get(\"Sentiment\", \"NEUTRAL\"),\n",
    "                    \"SentimentScore\": ent.get(\"SentimentScore\", {})\n",
    "                }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        resp = comprehend.detect_sentiment(Text=text_en, LanguageCode=\"en\")\n",
    "        return resp\n",
    "    except Exception:\n",
    "        return {\"Sentiment\": \"UNKNOWN\", \"SentimentScore\": {}}\n",
    "\n",
    "# =========================================================\n",
    "# FETCHERS\n",
    "# =========================================================\n",
    "\n",
    "def reddit_pushshift_generic(endpoints: List[str], query: str, size: int,\n",
    "                             after: Optional[Any] = None,\n",
    "                             before: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    for ep in endpoints:\n",
    "        try:\n",
    "            params = {\"q\": query, \"size\": size, \"sort\": \"desc\"}\n",
    "            if after is not None:\n",
    "                params[\"after\"] = after\n",
    "            if before is not None:\n",
    "                params[\"before\"] = before\n",
    "            r = requests.get(ep, params=params, timeout=20)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json().get(\"data\", [])\n",
    "                if data:\n",
    "                    return data\n",
    "        except Exception as e:\n",
    "            print(f\"[REDDIT] endpoint {ep} error: {e}\")\n",
    "    return []\n",
    "\n",
    "def reddit_submissions(query: str, size: int = 30, after=None, before=None) -> List[Dict[str, Any]]:\n",
    "    return reddit_pushshift_generic(PUSHSHIFT_SUBMISSIONS, query, size, after, before)\n",
    "\n",
    "def reddit_comments(query: str, size: int = 30, after=None, before=None) -> List[Dict[str, Any]]:\n",
    "    return reddit_pushshift_generic(PUSHSHIFT_COMMENTS, query, size, after, before)\n",
    "\n",
    "def hn_search(query: str, hits_per_page: int = 30) -> List[Dict[str, Any]]:\n",
    "    url = \"https://hn.algolia.com/api/v1/search\"\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            url,\n",
    "            params={\"query\": query, \"tags\": \"story\", \"hitsPerPage\": hits_per_page},\n",
    "            timeout=15\n",
    "        )\n",
    "        if r.status_code == 200:\n",
    "            return r.json().get(\"hits\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"[HN] error: {e}\")\n",
    "    return []\n",
    "\n",
    "def is_hn_in_window(hit: Dict[str, Any], start_ts: Optional[int], end_ts: Optional[int], days_back: int) -> bool:\n",
    "    ts = hit.get(\"created_at_i\")\n",
    "    if not ts:\n",
    "        return False\n",
    "    if start_ts is not None and end_ts is not None:\n",
    "        return start_ts <= ts <= end_ts\n",
    "    dt = datetime.datetime.utcfromtimestamp(ts)\n",
    "    age = (datetime.datetime.utcnow() - dt).days\n",
    "    return age <= days_back\n",
    "\n",
    "def fetch_from_mastodon_instance(instance: str, query: str, limit: int = 30) -> List[str]:\n",
    "    url = f\"https://{instance}/api/v2/search\"\n",
    "    try:\n",
    "        r = requests.get(url, params={\"q\": query, \"limit\": limit, \"resolve\": \"true\"}, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            return []\n",
    "        data = r.json()\n",
    "        out = []\n",
    "        for st in data.get(\"statuses\", []):\n",
    "            content_html = st.get(\"content\", \"\")\n",
    "            soup = BeautifulSoup(content_html, \"html.parser\")\n",
    "            txt = soup.get_text(\" \", strip=True)\n",
    "            if txt:\n",
    "                out.append(txt)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"[MASTODON:{instance}] error: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_from_mastodon_all(query: str, limit: int = 30) -> List[str]:\n",
    "    posts = []\n",
    "    for inst in MASTODON_INSTANCES:\n",
    "        posts.extend(fetch_from_mastodon_instance(inst, query, limit=limit))\n",
    "    return posts[:limit]\n",
    "\n",
    "# =========================================================\n",
    "# MATCHING + SCORING\n",
    "# =========================================================\n",
    "\n",
    "def match_level(text: str, company_name: str, lex: Dict[str, List[str]]) -> str:\n",
    "    low = text.lower()\n",
    "\n",
    "    for t in lex[\"exact_mentions\"] + lex[\"lang_variants\"]:\n",
    "        if t.lower() in low:\n",
    "            return \"strong\"\n",
    "\n",
    "    has_company = company_name.lower() in low\n",
    "    has_policy = any(pk.lower() in low for pk in lex[\"policy_terms\"])\n",
    "    has_noise = any(no.lower() in low for no in lex[\"anti_noise\"])\n",
    "\n",
    "    if has_company and has_policy:\n",
    "        return \"policy\"\n",
    "    if has_company and not has_policy:\n",
    "        return \"very_weak\" if has_noise else \"weak\"\n",
    "    if has_policy:\n",
    "        return \"weak\"\n",
    "    return \"very_weak\"\n",
    "\n",
    "def level_weight(level: str) -> float:\n",
    "    if level == \"strong\":\n",
    "        return WEIGHT_STRONG\n",
    "    if level == \"policy\":\n",
    "        return WEIGHT_POLICY\n",
    "    if level == \"weak\":\n",
    "        return WEIGHT_WEAK\n",
    "    return WEIGHT_VERY_WEAK\n",
    "\n",
    "def compute_relevance(level: str, source: str, temporal_mode: str) -> float:\n",
    "    \"\"\"\n",
    "    Score simple pour trier les posts.\n",
    "    - strong très haut\n",
    "    - policy haut\n",
    "    - + bonus si on est dans la fenêtre de la loi\n",
    "    - + petit bonus si source \"structurée\"\n",
    "    \"\"\"\n",
    "    base = level_weight(level)\n",
    "    if level == \"strong\":\n",
    "        base += 1.0\n",
    "    elif level == \"policy\":\n",
    "        base += 0.5\n",
    "\n",
    "    if temporal_mode == \"law_date\":\n",
    "        base += 0.3\n",
    "\n",
    "    if source in (\"reddit_submission\", \"hackernews\"):\n",
    "        base += 0.2\n",
    "\n",
    "    return base\n",
    "\n",
    "def hash_text(text: str) -> str:\n",
    "    return md5(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "# =========================================================\n",
    "# CSV\n",
    "# =========================================================\n",
    "\n",
    "def write_social_csv(directive_slug: str, rows: List[Dict[str, Any]]):\n",
    "    base_dir = os.path.join(OUTPUT_BASE, directive_slug)\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    path = os.path.join(base_dir, f\"social_{directive_slug}.csv\")\n",
    "    pd.DataFrame(rows).to_csv(path, index=False)\n",
    "    print(f\"[CSV] social -> {path}\")\n",
    "\n",
    "def write_analysis_csv(directive_slug: str, rows: List[Dict[str, Any]]):\n",
    "    base_dir = os.path.join(OUTPUT_BASE, directive_slug)\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    path = os.path.join(base_dir, f\"analysis_{directive_slug}.csv\")\n",
    "    pd.DataFrame(rows).to_csv(path, index=False)\n",
    "    print(f\"[CSV] analysis -> {path}\")\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "\n",
    "def main():\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] start social multi-source (temporal v3 LLM date + cap) ...\")\n",
    "\n",
    "    directives = list_directives(DIRECTIVES_DIR)\n",
    "    if not directives:\n",
    "        print(\"[!] no directive found\")\n",
    "        return\n",
    "\n",
    "    directive_path = directives[3]\n",
    "    directive_text = read_directive_text(directive_path)\n",
    "    directive_lang = detect_language(directive_text)\n",
    "    directive_title = guess_title_from_filename(directive_path)\n",
    "    directive_slug = slugify(directive_title)\n",
    "\n",
    "    # 1) lexique dynamique\n",
    "    lexicon = build_dynamic_lexicon(directive_title, directive_text, directive_lang)\n",
    "    print(f\"[DIR] {directive_title} | lang={directive_lang}\")\n",
    "    print(f\"[DIR] lexicon = {json.dumps(lexicon, indent=2)}\")\n",
    "\n",
    "    # 2) fenêtre temporelle via LLM DATES\n",
    "    pushshift_after, pushshift_before, hn_start_ts, hn_end_ts, effective_mode = compute_temporal_window_via_llm(\n",
    "        directive_title, directive_text, directive_lang\n",
    "    )\n",
    "    print(f\"[TIME] mode={effective_mode} pushshift_after={pushshift_after} pushshift_before={pushshift_before} hn=({hn_start_ts},{hn_end_ts})\")\n",
    "\n",
    "    # 3) entreprises\n",
    "    companies = load_companies(SP500_PATH, limit=MAX_COMPANIES)\n",
    "    print(f\"[SP500] loaded {len(companies)} companies\")\n",
    "\n",
    "    social_rows: List[Dict[str, Any]] = []\n",
    "    analysis_rows: List[Dict[str, Any]] = []\n",
    "    seen_hashes = set()\n",
    "\n",
    "    for idx, company in enumerate(companies, start=1):\n",
    "        name = company[\"name\"]\n",
    "        ticker = company[\"ticker\"]\n",
    "        print(f\"[{idx}/{len(companies)}] company={name} ({ticker})\")\n",
    "\n",
    "        queries = build_queries_for_company(lexicon, name, ticker)\n",
    "\n",
    "        # on collecte dans une liste locale\n",
    "        company_posts: List[Dict[str, Any]] = []\n",
    "        strategies_success: List[str] = []\n",
    "        strategies_failed: List[str] = []\n",
    "\n",
    "        # ============== PASS 1 : FENÊTRE LOI ==============\n",
    "        for q in queries:\n",
    "            # early stop souple\n",
    "            if len(company_posts) >= int(MAX_POSTS_PER_COMPANY * EARLY_STOP_FACTOR):\n",
    "                print(f\"[{name}] early-stop (law_date) at {len(company_posts)} candidates\")\n",
    "                break\n",
    "\n",
    "            # REDDIT\n",
    "            if ENABLE_REDDIT:\n",
    "                subms = reddit_submissions(q, size=POSTS_PER_QUERY,\n",
    "                                           after=pushshift_after,\n",
    "                                           before=pushshift_before)\n",
    "                comms = reddit_comments(q, size=POSTS_PER_QUERY,\n",
    "                                        after=pushshift_after,\n",
    "                                        before=pushshift_before)\n",
    "                if subms or comms:\n",
    "                    strategies_success.append(f\"reddit:{q} (law_date)\")\n",
    "                else:\n",
    "                    strategies_failed.append(f\"reddit:{q} (law_date)\")\n",
    "\n",
    "                for s in subms:\n",
    "                    full = (s.get(\"title\", \"\") + \" \" + s.get(\"selftext\", \"\")).strip()\n",
    "                    if not full:\n",
    "                        continue\n",
    "                    h = hash_text(\"REDDIT_SUBM|\" + full)\n",
    "                    if h in seen_hashes:\n",
    "                        continue\n",
    "                    seen_hashes.add(h)\n",
    "\n",
    "                    level = match_level(full, name, lexicon)\n",
    "                    sent = detect_targeted_or_doc_sentiment(full, name)\n",
    "                    label = sent.get(\"Sentiment\", \"UNKNOWN\").upper()\n",
    "                    scores = sent.get(\"SentimentScore\", {})\n",
    "                    rel = compute_relevance(level, \"reddit_submission\", \"law_date\")\n",
    "\n",
    "                    company_posts.append({\n",
    "                        \"directive_slug\": directive_slug,\n",
    "                        \"directive_title\": directive_title,\n",
    "                        \"company\": name,\n",
    "                        \"ticker\": ticker,\n",
    "                        \"query\": q,\n",
    "                        \"source\": \"reddit_submission\",\n",
    "                        \"text\": full,\n",
    "                        \"match_level\": level,\n",
    "                        \"sentiment\": label,\n",
    "                        \"sentiment_score\": scores,\n",
    "                        \"temporal_mode\": \"law_date\",\n",
    "                        \"relevance_score\": rel,\n",
    "                    })\n",
    "\n",
    "                for c in comms:\n",
    "                    body = c.get(\"body\", \"\")\n",
    "                    if not body or body == \"[deleted]\":\n",
    "                        continue\n",
    "                    h = hash_text(\"REDDIT_COMM|\" + body)\n",
    "                    if h in seen_hashes:\n",
    "                        continue\n",
    "                    seen_hashes.add(h)\n",
    "\n",
    "                    level = match_level(body, name, lexicon)\n",
    "                    sent = detect_targeted_or_doc_sentiment(body, name)\n",
    "                    label = sent.get(\"Sentiment\", \"UNKNOWN\").upper()\n",
    "                    scores = sent.get(\"SentimentScore\", {})\n",
    "                    rel = compute_relevance(level, \"reddit_comment\", \"law_date\")\n",
    "\n",
    "                    company_posts.append({\n",
    "                        \"directive_slug\": directive_slug,\n",
    "                        \"directive_title\": directive_title,\n",
    "                        \"company\": name,\n",
    "                        \"ticker\": ticker,\n",
    "                        \"query\": q,\n",
    "                        \"source\": \"reddit_comment\",\n",
    "                        \"text\": body,\n",
    "                        \"match_level\": level,\n",
    "                        \"sentiment\": label,\n",
    "                        \"sentiment_score\": scores,\n",
    "                        \"temporal_mode\": \"law_date\",\n",
    "                        \"relevance_score\": rel,\n",
    "                    })\n",
    "\n",
    "            # HN\n",
    "            if ENABLE_HN:\n",
    "                hn_hits = hn_search(q, hits_per_page=POSTS_PER_QUERY)\n",
    "                filtered_hn = []\n",
    "                for hhit in hn_hits:\n",
    "                    if is_hn_in_window(hhit, hn_start_ts, hn_end_ts, DAYS_BACK):\n",
    "                        filtered_hn.append(hhit)\n",
    "\n",
    "                if filtered_hn:\n",
    "                    strategies_success.append(f\"hn:{q} (law_date)\")\n",
    "                else:\n",
    "                    strategies_failed.append(f\"hn:{q} (law_date)\")\n",
    "\n",
    "                for hhit in filtered_hn:\n",
    "                    title = hhit.get(\"title\", \"\")\n",
    "                    url = hhit.get(\"url\", \"\") or \"\"\n",
    "                    txt = title\n",
    "                    h = hash_text(\"HN|\" + txt + url)\n",
    "                    if h in seen_hashes:\n",
    "                        continue\n",
    "                    seen_hashes.add(h)\n",
    "\n",
    "                    level = match_level(txt, name, lexicon)\n",
    "                    sent = detect_targeted_or_doc_sentiment(txt, name)\n",
    "                    label = sent.get(\"Sentiment\", \"UNKNOWN\").upper()\n",
    "                    scores = sent.get(\"SentimentScore\", {})\n",
    "                    rel = compute_relevance(level, \"hackernews\", \"law_date\")\n",
    "\n",
    "                    company_posts.append({\n",
    "                        \"directive_slug\": directive_slug,\n",
    "                        \"directive_title\": directive_title,\n",
    "                        \"company\": name,\n",
    "                        \"ticker\": ticker,\n",
    "                        \"query\": q,\n",
    "                        \"source\": \"hackernews\",\n",
    "                        \"text\": txt,\n",
    "                        \"match_level\": level,\n",
    "                        \"sentiment\": label,\n",
    "                        \"sentiment_score\": scores,\n",
    "                        \"url\": url,\n",
    "                        \"temporal_mode\": \"law_date\",\n",
    "                        \"relevance_score\": rel,\n",
    "                    })\n",
    "\n",
    "            # Mastodon\n",
    "            if ENABLE_MASTODON:\n",
    "                m_posts = fetch_from_mastodon_all(q, limit=POSTS_PER_QUERY)\n",
    "                if m_posts:\n",
    "                    strategies_success.append(f\"mastodon:{q}\")\n",
    "                else:\n",
    "                    strategies_failed.append(f\"mastodon:{q}\")\n",
    "\n",
    "                for p in m_posts:\n",
    "                    h = hash_text(\"MASTODON|\" + p)\n",
    "                    if h in seen_hashes:\n",
    "                        continue\n",
    "                    seen_hashes.add(h)\n",
    "\n",
    "                    level = match_level(p, name, lexicon)\n",
    "                    sent = detect_targeted_or_doc_sentiment(p, name)\n",
    "                    label = sent.get(\"Sentiment\", \"UNKNOWN\").upper()\n",
    "                    scores = sent.get(\"SentimentScore\", {})\n",
    "                    rel = compute_relevance(level, \"mastodon\", \"law_date\")\n",
    "\n",
    "                    company_posts.append({\n",
    "                        \"directive_slug\": directive_slug,\n",
    "                        \"directive_title\": directive_title,\n",
    "                        \"company\": name,\n",
    "                        \"ticker\": ticker,\n",
    "                        \"query\": q,\n",
    "                        \"source\": \"mastodon\",\n",
    "                        \"text\": p,\n",
    "                        \"match_level\": level,\n",
    "                        \"sentiment\": label,\n",
    "                        \"sentiment_score\": scores,\n",
    "                        \"temporal_mode\": \"law_date\",\n",
    "                        \"relevance_score\": rel,\n",
    "                    })\n",
    "\n",
    "            time.sleep(0.12)\n",
    "\n",
    "        # ============== PASS 2 : FALLBACK RECENT ==============\n",
    "        if not company_posts:\n",
    "            print(f\"[{name}] no posts in law_date window -> recent fallback {DAYS_BACK}d\")\n",
    "            for q in queries:\n",
    "                if len(company_posts) >= int(MAX_POSTS_PER_COMPANY * EARLY_STOP_FACTOR):\n",
    "                    print(f\"[{name}] early-stop (recent) at {len(company_posts)} candidates\")\n",
    "                    break\n",
    "\n",
    "                if ENABLE_REDDIT:\n",
    "                    subms = reddit_submissions(q, size=POSTS_PER_QUERY,\n",
    "                                               after=PUSHSHIFT_RECENT_AFTER,\n",
    "                                               before=None)\n",
    "                    comms = reddit_comments(q, size=POSTS_PER_QUERY,\n",
    "                                            after=PUSHSHIFT_RECENT_AFTER,\n",
    "                                            before=None)\n",
    "                    for s in subms:\n",
    "                        full = (s.get(\"title\", \"\") + \" \" + s.get(\"selftext\", \"\")).strip()\n",
    "                        if not full:\n",
    "                            continue\n",
    "                        h = hash_text(\"REDDIT_SUBM_RECENT|\" + full)\n",
    "                        if h in seen_hashes:\n",
    "                            continue\n",
    "                        seen_hashes.add(h)\n",
    "\n",
    "                        level = match_level(full, name, lexicon)\n",
    "                        sent = detect_targeted_or_doc_sentiment(full, name)\n",
    "                        label = sent.get(\"Sentiment\", \"UNKNOWN\").upper()\n",
    "                        scores = sent.get(\"SentimentScore\", {})\n",
    "                        rel = compute_relevance(level, \"reddit_submission\", \"recent_fallback\")\n",
    "\n",
    "                        company_posts.append({\n",
    "                            \"directive_slug\": directive_slug,\n",
    "                            \"directive_title\": directive_title,\n",
    "                            \"company\": name,\n",
    "                            \"ticker\": ticker,\n",
    "                            \"query\": q,\n",
    "                            \"source\": \"reddit_submission\",\n",
    "                            \"text\": full,\n",
    "                            \"match_level\": level,\n",
    "                            \"sentiment\": label,\n",
    "                            \"sentiment_score\": scores,\n",
    "                            \"temporal_mode\": \"recent_fallback\",\n",
    "                            \"relevance_score\": rel,\n",
    "                        })\n",
    "\n",
    "                    for c in comms:\n",
    "                        body = c.get(\"body\", \"\")\n",
    "                        if not body or body == \"[deleted]\":\n",
    "                            continue\n",
    "                        h = hash_text(\"REDDIT_COMM_RECENT|\" + body)\n",
    "                        if h in seen_hashes:\n",
    "                            continue\n",
    "                        seen_hashes.add(h)\n",
    "\n",
    "                        level = match_level(body, name, lexicon)\n",
    "                        sent = detect_targeted_or_doc_sentiment(body, name)\n",
    "                        label = sent.get(\"Sentiment\", \"UNKNOWN\").upper()\n",
    "                        scores = sent.get(\"SentimentScore\", {})\n",
    "                        rel = compute_relevance(level, \"reddit_comment\", \"recent_fallback\")\n",
    "\n",
    "                        company_posts.append({\n",
    "                            \"directive_slug\": directive_slug,\n",
    "                            \"directive_title\": directive_title,\n",
    "                            \"company\": name,\n",
    "                            \"ticker\": ticker,\n",
    "                            \"query\": q,\n",
    "                            \"source\": \"reddit_comment\",\n",
    "                            \"text\": body,\n",
    "                            \"match_level\": level,\n",
    "                            \"sentiment\": label,\n",
    "                            \"sentiment_score\": scores,\n",
    "                            \"temporal_mode\": \"recent_fallback\",\n",
    "                            \"relevance_score\": rel,\n",
    "                        })\n",
    "\n",
    "                if ENABLE_HN:\n",
    "                    hn_hits = hn_search(q, hits_per_page=POSTS_PER_QUERY)\n",
    "                    for hhit in hn_hits:\n",
    "                        if not is_hn_in_window(hhit, None, None, DAYS_BACK):\n",
    "                            continue\n",
    "                        title = hhit.get(\"title\", \"\")\n",
    "                        url = hhit.get(\"url\", \"\") or \"\"\n",
    "                        txt = title\n",
    "                        h = hash_text(\"HN_RECENT|\" + txt + url)\n",
    "                        if h in seen_hashes:\n",
    "                            continue\n",
    "                        seen_hashes.add(h)\n",
    "\n",
    "                        level = match_level(txt, name, lexicon)\n",
    "                        sent = detect_targeted_or_doc_sentiment(txt, name)\n",
    "                        label = sent.get(\"Sentiment\", \"UNKNOWN\").upper()\n",
    "                        scores = sent.get(\"SentimentScore\", {})\n",
    "                        rel = compute_relevance(level, \"hackernews\", \"recent_fallback\")\n",
    "\n",
    "                        company_posts.append({\n",
    "                            \"directive_slug\": directive_slug,\n",
    "                            \"directive_title\": directive_title,\n",
    "                            \"company\": name,\n",
    "                            \"ticker\": ticker,\n",
    "                            \"query\": q,\n",
    "                            \"source\": \"hackernews\",\n",
    "                            \"text\": txt,\n",
    "                            \"match_level\": level,\n",
    "                            \"sentiment\": label,\n",
    "                            \"sentiment_score\": scores,\n",
    "                            \"url\": url,\n",
    "                            \"temporal_mode\": \"recent_fallback\",\n",
    "                            \"relevance_score\": rel,\n",
    "                        })\n",
    "\n",
    "                time.sleep(0.12)\n",
    "\n",
    "        # ======== SEULEMENT MAINTENANT : ON TRIE & ON COUPE ========\n",
    "        # tri décroissant par pertinence\n",
    "        company_posts_sorted = sorted(company_posts, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "        company_posts_kept = company_posts_sorted[:MAX_POSTS_PER_COMPANY]\n",
    "\n",
    "        # recalcul agrégats sur ce qu'on garde\n",
    "        nb_posts_found = len(company_posts_kept)\n",
    "        counts_by_level = {\"strong\": 0, \"policy\": 0, \"weak\": 0, \"very_weak\": 0}\n",
    "        agg_pos = agg_neg = agg_neu = agg_mix = 0.0\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for p in company_posts_kept:\n",
    "            lvl = p[\"match_level\"]\n",
    "            counts_by_level[lvl] += 1\n",
    "            scores = p[\"sentiment_score\"] or {}\n",
    "            w = level_weight(lvl)\n",
    "            agg_pos += float(scores.get(\"Positive\", 0.0)) * w\n",
    "            agg_neg += float(scores.get(\"Negative\", 0.0)) * w\n",
    "            agg_neu += float(scores.get(\"Neutral\", 0.0)) * w\n",
    "            agg_mix += float(scores.get(\"Mixed\", 0.0)) * w\n",
    "            total_weight += w\n",
    "\n",
    "        if total_weight > 0:\n",
    "            avg_pos = agg_pos / total_weight\n",
    "            avg_neg = agg_neg / total_weight\n",
    "            avg_neu = agg_neu / total_weight\n",
    "            avg_mix = agg_mix / total_weight\n",
    "        else:\n",
    "            avg_pos = avg_neg = avg_mix = 0.0\n",
    "            avg_neu = 1.0\n",
    "\n",
    "        # on pousse les posts gardés dans le CSV global\n",
    "        for p in company_posts_kept:\n",
    "            # on convertit le dict sentiment_score en json string pour csv\n",
    "            p_out = dict(p)\n",
    "            p_out[\"sentiment_score\"] = json.dumps(p[\"sentiment_score\"])\n",
    "            social_rows.append(p_out)\n",
    "\n",
    "        analysis_rows.append({\n",
    "            \"directive_slug\": directive_slug,\n",
    "            \"directive_title\": directive_title,\n",
    "            \"company\": name,\n",
    "            \"ticker\": ticker,\n",
    "            \"nb_posts_found\": nb_posts_found,\n",
    "            \"nb_posts_strong\": counts_by_level[\"strong\"],\n",
    "            \"nb_posts_policy\": counts_by_level[\"policy\"],\n",
    "            \"nb_posts_weak\": counts_by_level[\"weak\"],\n",
    "            \"nb_posts_very_weak\": counts_by_level[\"very_weak\"],\n",
    "            \"avg_pos\": round(avg_pos, 4),\n",
    "            \"avg_neg\": round(avg_neg, 4),\n",
    "            \"avg_neu\": round(avg_neu, 4),\n",
    "            \"avg_mix\": round(avg_mix, 4),\n",
    "            \"strategies_success\": \" | \".join(strategies_success),\n",
    "            \"strategies_failed\": \" | \".join(strategies_failed),\n",
    "            \"temporal_mode\": effective_mode,\n",
    "        })\n",
    "\n",
    "    write_social_csv(directive_slug, social_rows)\n",
    "    write_analysis_csv(directive_slug, analysis_rows)\n",
    "    print(\"[OK] done (temporal v3 LLM date + cap).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
