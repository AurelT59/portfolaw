{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e2d30a5-3261-4ff5-9967-4c5a4b1e1b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'sec-parser'...\n",
      "remote: Enumerating objects: 5000, done.\u001b[K\n",
      "remote: Counting objects: 100% (582/582), done.\u001b[K\n",
      "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
      "remote: Total 5000 (delta 518), reused 488 (delta 488), pack-reused 4418 (from 2)\u001b[K\n",
      "Receiving objects: 100% (5000/5000), 2.59 MiB | 260.00 KiB/s, done.\n",
      "Resolving deltas: 100% (3556/3556), done.\n",
      "Updating files: 100% (224/224), done.\n",
      "/mnt/custom-file-systems/s3/shared/company_profile/sec-parser\n",
      "Processing /mnt/custom-file-systems/s3/shared/company_profile/sec-parser\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.11/site-packages (from sec-parser==0.58.1) (4.14.2)\n",
      "Requirement already satisfied: cssutils<3.0.0,>=2.11.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser==0.58.1) (2.11.1)\n",
      "Requirement already satisfied: frozendict<3.0.0,>=2.4.4 in /opt/conda/lib/python3.11/site-packages (from sec-parser==0.58.1) (2.4.6)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /opt/conda/lib/python3.11/site-packages (from sec-parser==0.58.1) (0.7.3)\n",
      "Requirement already satisfied: lxml<6.0.0,>=5.2.2 in /opt/conda/lib/python3.11/site-packages (from sec-parser==0.58.1) (5.4.0)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.2.2 in /opt/conda/lib/python3.11/site-packages (from sec-parser==0.58.1) (2.3.3)\n",
      "Requirement already satisfied: sec-downloader<0.12.0,>=0.11.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser==0.58.1) (0.11.2)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from sec-parser==0.58.1) (0.9.0)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.4.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser==0.58.1) (3.6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->sec-parser==0.58.1) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->sec-parser==0.58.1) (4.15.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.11/site-packages (from cssutils<3.0.0,>=2.11.1->sec-parser==0.58.1) (10.8.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.2->sec-parser==0.58.1) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.2->sec-parser==0.58.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.2->sec-parser==0.58.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.2->sec-parser==0.58.1) (2025.2)\n",
      "Requirement already satisfied: sec-edgar-downloader in /opt/conda/lib/python3.11/site-packages (from sec-downloader<0.12.0,>=0.11.1->sec-parser==0.58.1) (5.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.2->sec-parser==0.58.1) (1.17.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser==0.58.1) (2.32.5)\n",
      "Requirement already satisfied: pyrate-limiter>=3.6.0 in /opt/conda/lib/python3.11/site-packages (from sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser==0.58.1) (3.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser==0.58.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser==0.58.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser==0.58.1) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser==0.58.1) (2025.10.5)\n",
      "Building wheels for collected packages: sec-parser\n",
      "  Building wheel for sec-parser (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sec-parser: filename=sec_parser-0.58.1-py3-none-any.whl size=77056 sha256=bcef295d4523328f2b2d6f51e131151f92167131cd602041cd723aef7a5e3f38\n",
      "  Stored in directory: /home/sagemaker-user/.cache/pip/wheels/ba/09/59/9570844b0ca15289686c7d6c0785fb418945f49ef123c8336c\n",
      "Successfully built sec-parser\n",
      "Installing collected packages: sec-parser\n",
      "Successfully installed sec-parser-0.58.1\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/alphanome-ai/sec-parser.git\n",
    "%cd sec-parser\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f5c606a-f12b-497c-ab0f-5c663d508aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: sec-parser 0.58.1\n",
      "Uninstalling sec-parser-0.58.1:\n",
      "  Successfully uninstalled sec-parser-0.58.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y sec-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a3c8265-ef41-47ca-9e88-5cb338c2d995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: sec-parser\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip show sec-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c0e78e0-6898-4bb8-93f9-58774335dcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sec-parser\n",
      "  Using cached sec_parser-0.58.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (4.14.2)\n",
      "Requirement already satisfied: cssutils<3.0.0,>=2.11.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (2.11.1)\n",
      "Requirement already satisfied: frozendict<3.0.0,>=2.4.4 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (2.4.6)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (0.7.3)\n",
      "Requirement already satisfied: lxml<6.0.0,>=5.2.2 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (5.4.0)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.2.2 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (2.3.3)\n",
      "Requirement already satisfied: sec-downloader<0.12.0,>=0.11.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (0.11.2)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (0.9.0)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.4.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (3.6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->sec-parser) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->sec-parser) (4.15.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.11/site-packages (from cssutils<3.0.0,>=2.11.1->sec-parser) (10.8.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.2->sec-parser) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.2->sec-parser) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.2->sec-parser) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.2->sec-parser) (2025.2)\n",
      "Requirement already satisfied: sec-edgar-downloader in /opt/conda/lib/python3.11/site-packages (from sec-downloader<0.12.0,>=0.11.1->sec-parser) (5.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.2->sec-parser) (1.17.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (2.32.5)\n",
      "Requirement already satisfied: pyrate-limiter>=3.6.0 in /opt/conda/lib/python3.11/site-packages (from sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (3.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (2025.10.5)\n",
      "Using cached sec_parser-0.58.1-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: sec-parser\n",
      "Successfully installed sec-parser-0.58.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sec-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ad643-1939-4e83-a71d-3e9c7e0235e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_parser import Edgar10KParser\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "root = \"/home/sagemaker-user/shared\"\n",
    "input_dir = os.path.join(root, \"fillings\")\n",
    "output_dir = os.path.join(root, \"company_profile\", \"extracted_sections\")\n",
    "\n",
    "parser = Edgar10KParser()\n",
    "\n",
    "# Parcourir chaque entreprise\n",
    "for company_name in os.listdir(input_dir):\n",
    "    company_path = os.path.join(input_dir, company_name)\n",
    "    if not os.path.isdir(company_path):\n",
    "        continue  # Ignorer les fichiers, ne prendre que les dossiers\n",
    "\n",
    "    # Chercher le fichier 10-K dans le dossier de l'entreprise\n",
    "    tenk_files = [f for f in os.listdir(company_path) if f.lower().endswith(\".html\")]\n",
    "    if not tenk_files:\n",
    "        print(f\"Aucun fichier 10-K trouv√© pour {company_name}\")\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(company_path, tenk_files[0])  # Prendre le premier HTML trouv√©\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "\n",
    "    # Parser le HTML\n",
    "    elements = parser.parse(html)\n",
    "    print(elements)\n",
    "    # Cr√©er le dossier de sortie pour cette entreprise\n",
    "    company_output_folder = os.path.join(output_dir, company_name)\n",
    "    os.makedirs(company_output_folder, exist_ok=True)\n",
    "\n",
    "    current_section = None\n",
    "    current_content = []\n",
    "\n",
    "    for element in elements:\n",
    "        # D√©tecter TopSectionTitle\n",
    "        if \"TopSectionTitle\" in element.__class__.__name__:\n",
    "            # Sauvegarder la section pr√©c√©dente\n",
    "            if current_section and current_content:\n",
    "                short_title = re.sub(r'[\\\\/*?:\"<>|]', \"_\", current_section[:30])\n",
    "                file_name = f\"{short_title}.json\"\n",
    "                with open(os.path.join(company_output_folder, file_name), \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump({\"content\": current_content}, f, indent=4, ensure_ascii=False)\n",
    "            # Nouvelle section\n",
    "            current_section = element.text\n",
    "            current_content = []\n",
    "        else:\n",
    "            # Garder TitleElement et TextElement intacts\n",
    "            current_content.append({\n",
    "                \"type\": element.__class__.__name__.replace(\"Element\", \"\"),\n",
    "                \"text\": element.text\n",
    "            })\n",
    "\n",
    "    # Sauvegarder la derni√®re section\n",
    "    if current_section and current_content:\n",
    "        file_name = re.sub(r'[\\\\/*?:\"<>|]', \"_\", current_section) + \".json\"\n",
    "        with open(os.path.join(company_output_folder, file_name), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"content\": current_content}, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095a643-d764-44f6-9229-2ba46a6ed136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import re\n",
    "\n",
    "# --- Config AWS Bedrock ---\n",
    "config = Config(\n",
    "    region_name=\"us-west-2\",\n",
    "    connect_timeout=30,\n",
    "    read_timeout=300,  # permet jusqu‚Äô√† 5 min par requ√™te\n",
    ")\n",
    "bedrock = boto3.client(\"bedrock-runtime\", config=config)\n",
    "\n",
    "# --- R√©pertoires et filtres ---\n",
    "input_dir = \"/home/sagemaker-user/shared/company_profile/extracted_sections/BRK-B\"\n",
    "selected_items = [\"item 1\", \"item 1a\", \"item 1c\", \"item 2\", \"item 3\", \"item 7\", \"item 7a\", \"item 8\"]\n",
    "not_selected_items = [\"item 1b\", \"item 10\", \"item 11\", \"item 12\", \"item 13\", \"item 14\", \"item 15\", \"item 16\"]\n",
    "fillings_dir = \"/home/sagemaker-user/shared/fillings\"\n",
    "\n",
    "\n",
    "merged_data = {}\n",
    "\n",
    "# --- Fusion des fichiers s√©lectionn√©s ---\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        name = os.path.splitext(filename)[0].lower().strip()  # \"item 1.business\"\n",
    "        main_name = name.split(\".\")[0]  # \"item 1\"\n",
    "\n",
    "        # V√©rifie inclusion/exclusion\n",
    "        if main_name in selected_items and main_name not in not_selected_items:\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            merged_data[filename] = data\n",
    "\n",
    "\n",
    "# --- Fallback to full 10-K if any selected item is missing ---\n",
    "if len(merged_data) < len(selected_items):\n",
    "    filing_path = os.path.join(fillings_dir, company_name)\n",
    "    if os.path.isdir(filing_path):\n",
    "        for file in os.listdir(filing_path):\n",
    "            if file.endswith(\".html\") or file.endswith(\".txt\"):\n",
    "                with open(os.path.join(filing_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                merged_data = {\"full_10k\": content}\n",
    "                break\n",
    "\n",
    "# --- Fusion du texte ---\n",
    "merged_text = \"\\n\\n\".join(\n",
    "    f\"--- {section} ---\\n{json.dumps(content, ensure_ascii=False)}\"\n",
    "    for section, content in merged_data.items()\n",
    ")\n",
    "\n",
    "print(f\"Taille du texte fusionn√© : {len(merged_text):,} caract√®res\")\n",
    "\n",
    "for key, values in merged_data.items():\n",
    "    print(key)\n",
    "\n",
    "# --- Fonction utilitaire pour d√©couper le texte ---\n",
    "def chunk_text(text, max_length=200000):\n",
    "    \"\"\"D√©coupe un texte long en morceaux d‚Äôenviron max_length caract√®res.\"\"\"\n",
    "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "chunks = chunk_text(merged_text)\n",
    "\n",
    "# --- Prompt principal (r√©utilis√© sur chaque chunk) ---\n",
    "base_prompt = \"\"\"\n",
    "You are an expert financial analyst specializing in 10-K report analysis.\n",
    "\n",
    "I want you to focus on several points:\n",
    "Give the name of companies mentionned in the text and their relationship with the company of the report.\n",
    "Where are the operations of the company ? What can you say about the ESG and R&D policies ?\n",
    "\n",
    "From the following text, create a **concise JSON ** and try to complete the most keys you can.\n",
    "Include measurement units for quantitative data and return **nothing else but the JSON**.\n",
    "\n",
    "Expected structure:\n",
    "{\n",
    "  \"date\": \"...\",\n",
    "  \"name\": \"...\",\n",
    "  \"industry\": \"...\",\n",
    "  \"sub_industry\": \"...\",\n",
    "  \"customer_segmentation\": [\"...\"],\n",
    "  \"products\": [\"...\"],\n",
    "  \"supplier_countries\": [\"...\"],\n",
    "  \"supply_chain\": \"...\",\n",
    "  \"geographic_market_segment\": [\"...\"],\n",
    "  \"related_companies\": [{\"company_name\": \"...\", \"relationship_type\": \"...\"}],\n",
    "  \"competitors\": [\"...\"],\n",
    "  \"substitute_products\": [\"...\"],\n",
    "  \"revenue\": {\"value\": null, \"unit\": \"...\", \"variation\": null},\n",
    "  \"net_income\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"gross_margin\": {\"value\": null, \"unit\": \"%\"},\n",
    "  \"income_tax_expense\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"share_buybacks\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"dividends\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"debt\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"interest_expense\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"depreciation\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"free_cash_flow\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"total_assets\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"shareholders_equity\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"ongoing_litigation\": [\"...\"],\n",
    "  \"research_development_expense\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"research_development_policy\": [\"...\"],\n",
    "  \"ESG_policy\": [\"...\"]\n",
    "}\n",
    "\n",
    "Most important, return **nothing else but the JSON in the right format**.\n",
    "\"\"\"\n",
    "\n",
    "# --- Fonction d‚Äôappel Claude ---\n",
    "def call_claude(prompt_text):\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2500,\n",
    "        \"temperature\": 0.1,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt_text}]}],\n",
    "    }\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        body=json.dumps(body),\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    return response_body[\"content\"][0][\"text\"]\n",
    "\n",
    "# --- Appels successifs et fusion JSON ---\n",
    "partial_results = []\n",
    "print(f\"\\nTraitement du bloc de {len(chunks)} chunks\")\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    prompt_chunk = f\"{base_prompt}\\n\\nSource text (part {i}/{len(chunks)}):\\n{chunk}\"\n",
    "\n",
    "    try:\n",
    "        json_text = call_claude(prompt_chunk).strip()\n",
    "        print(json_text[:500])  # voir ce qui est renvoy√©\n",
    "        if not json_text:\n",
    "            print(f\"Chunk {i}: r√©ponse vide, ignor√©.\")\n",
    "            continue\n",
    "\n",
    "        # Tente de d√©tecter du JSON m√™me si la r√©ponse contient du texte autour\n",
    "        match = re.search(r\"\\{.*\\}\", json_text, re.DOTALL)\n",
    "        if match:\n",
    "            json_text = match.group(0)\n",
    "        else:\n",
    "            print(f\"Chunk {i}: pas de JSON d√©tect√©.\")\n",
    "            continue\n",
    "\n",
    "        # Parse JSON\n",
    "        parsed = json.loads(json_text)\n",
    "        partial_results.append(parsed)\n",
    "        print(f\"Chunk {i} trait√© avec succ√®s.\")\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Chunk {i}: JSON invalide ({e})\")\n",
    "        print(f\"R√©ponse partielle : {json_text[:200]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur sur le chunk {i}: {e}\")\n",
    "\n",
    "\n",
    "# --- Fusion intelligente des JSON partiels ---\n",
    "final_json = {}\n",
    "\n",
    "for partial in partial_results:\n",
    "    for key, value in partial.items():\n",
    "        # Cl√© d√©j√† existante dans le JSON final\n",
    "        if key in final_json:\n",
    "            existing = final_json.get(key)\n",
    "\n",
    "            # Cas 1 : liste\n",
    "            if isinstance(value, list):\n",
    "                # Si liste de dictionnaires (ex: related_companies)\n",
    "                if all(isinstance(v, dict) for v in value):\n",
    "                    seen = set()\n",
    "                    merged_list = []\n",
    "                    for item in existing + value:\n",
    "                        identifier = json.dumps(item, sort_keys=True)\n",
    "                        if identifier not in seen:\n",
    "                            seen.add(identifier)\n",
    "                            merged_list.append(item)\n",
    "                    final_json[key] = merged_list\n",
    "                else:\n",
    "                    # Liste simple (texte, nombres, etc.)\n",
    "                    final_json[key] = list(set(existing + value))\n",
    "\n",
    "            # Cas 2 : dictionnaire num√©rique {\"value\": ..., \"unit\": ...}\n",
    "            elif isinstance(value, dict) and \"value\" in value:\n",
    "                if not existing.get(\"value\") and value.get(\"value\"):\n",
    "                    final_json[key] = value\n",
    "                elif not existing.get(\"unit\") and value.get(\"unit\"):\n",
    "                    existing[\"unit\"] = value[\"unit\"]\n",
    "                    final_json[key] = existing\n",
    "\n",
    "            # Cas 3 : champ simple (str, int, etc.)\n",
    "            else:\n",
    "                if not existing and value:\n",
    "                    final_json[key] = value\n",
    "\n",
    "        else:\n",
    "            # Premi√®re apparition de la cl√©\n",
    "            final_json[key] = value\n",
    "\n",
    "print(final_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5a79f-e651-4c7e-b228-ca54dd2b96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# --- Configuration AWS Bedrock ---\n",
    "config = Config(region_name=\"us-west-2\", connect_timeout=30, read_timeout=300)\n",
    "bedrock = boto3.client(\"bedrock-runtime\", config=config)\n",
    "\n",
    "# --- R√©pertoires racine ---\n",
    "root_dir = \"/home/sagemaker-user/shared/company_profile/extracted_sections\"\n",
    "output_root = \"/home/sagemaker-user/shared/company_profile/profiles\"\n",
    "\n",
    "# --- S√©lection et exclusion des sections ---\n",
    "selected_items = [\"item 1\", \"item 1a\", \"item 1c\", \"item 2\", \"item 3\", \"item 7\", \"item 7a\", \"item 8\"]\n",
    "not_selected_items = [\"item 1b\", \"item 10\", \"item 11\", \"item 12\", \"item 13\", \"item 14\", \"item 15\", \"item 16\"]\n",
    "\n",
    "# --- Fonction : d√©coupe un texte long ---\n",
    "def chunk_text(text, max_length=200000):\n",
    "    return [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "# --- Fonction : appel Claude ---\n",
    "def call_claude(prompt_text):\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2500,\n",
    "        \"temperature\": 0.1,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt_text}]}],\n",
    "    }\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        body=json.dumps(body),\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    return response_body[\"content\"][0][\"text\"]\n",
    "\n",
    "# --- Fusion intelligente de JSONs partiels ---\n",
    "def merge_partial_jsons(partials):\n",
    "    final_json = {}\n",
    "    for partial in partials:\n",
    "        for key, value in partial.items():\n",
    "            if key in final_json:\n",
    "                existing = final_json.get(key)\n",
    "\n",
    "                # üß© Fusion des listes\n",
    "                if isinstance(value, list):\n",
    "                    if not isinstance(existing, list):\n",
    "                        existing = [] if existing is None else [existing]\n",
    "                    # Si ce sont des dictionnaires, on d√©doublonne proprement\n",
    "                    if all(isinstance(v, dict) for v in value):\n",
    "                        seen = set()\n",
    "                        merged = []\n",
    "                        for item in existing + value:\n",
    "                            ident = json.dumps(item, sort_keys=True)\n",
    "                            if ident not in seen:\n",
    "                                seen.add(ident)\n",
    "                                merged.append(item)\n",
    "                        final_json[key] = merged\n",
    "                    else:\n",
    "                        final_json[key] = list(set(existing + value))\n",
    "\n",
    "                # üßÆ Fusion d‚Äôobjets num√©riques (dicts contenant \"value\")\n",
    "                elif isinstance(value, dict) and \"value\" in value:\n",
    "                    if not isinstance(existing, dict):\n",
    "                        existing = {}\n",
    "                    if \"value\" not in existing or not existing.get(\"value\"):\n",
    "                        existing[\"value\"] = value.get(\"value\")\n",
    "                    if \"unit\" not in existing or not existing.get(\"unit\"):\n",
    "                        existing[\"unit\"] = value.get(\"unit\")\n",
    "                    final_json[key] = existing\n",
    "\n",
    "                # üßæ Fusion des autres types simples\n",
    "                elif existing in (None, \"\", []):\n",
    "                    final_json[key] = value\n",
    "\n",
    "                # üß© Si les deux sont des cha√Ænes, on peut les concat√©ner\n",
    "                elif isinstance(existing, str) and isinstance(value, str):\n",
    "                    if value not in existing:\n",
    "                        final_json[key] = existing + \" \" + value\n",
    "            else:\n",
    "                final_json[key] = value\n",
    "    return final_json\n",
    "\n",
    "\n",
    "# --- Prompt principal ---\n",
    "base_prompt = \"\"\"\n",
    "You are an expert financial analyst specializing in 10-K report analysis.\n",
    "\n",
    "I want you to focus on several points:\n",
    "Give the name of companies mentionned in the text and their relationship with the company of the report.\n",
    "Where are the operations of the company ? What can you say about the ESG and R&D policies ?\n",
    "\n",
    "From the following text, create a **concise JSON** and try to complete the most keys you can.\n",
    "Include measurement units for quantitative data and return **nothing else but the JSON**.\n",
    "\n",
    "For all quantitative data, keep the original number formatting, including commas (e.g., ‚Äú3,450‚Äù instead of ‚Äú3450‚Äù).\n",
    "\n",
    "Expected structure:\n",
    "{\n",
    "  \"date\": \"...\",\n",
    "  \"name\": \"...\",\n",
    "  \"industry\": \"...\",\n",
    "  \"sub_industry\": \"...\",\n",
    "  \"customer_segmentation\": [\"...\"],\n",
    "  \"products\": [\"...\"],\n",
    "  \"supplier_countries\": [\"...\"],\n",
    "  \"supply_chain\": \"...\",\n",
    "  \"geographic_market_segment\": [\"...\"],\n",
    "  \"related_companies\": [{\"company_name\": \"...\", \"relationship_type\": \"...\"}],\n",
    "  \"competitors\": [\"...\"],\n",
    "  \"substitute_products\": [\"...\"],\n",
    "  \"revenue\": {\"value\": null, \"unit\": \"...\", \"variation\": null},\n",
    "  \"net_income\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"gross_margin\": {\"value\": null, \"unit\": \"%\"},\n",
    "  \"income_tax_expense\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"share_buybacks\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"dividends\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"debt\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"interest_expense\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"depreciation\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"free_cash_flow\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"total_assets\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"shareholders_equity\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"ongoing_litigation\": [\"...\"],\n",
    "  \"research_development_expense\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"research_development_policy\": [\"...\"],\n",
    "  \"ESG_policy\": [\"...\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# --- Liste des entreprises √† traiter plus tard ---\n",
    "pending_companies = []\n",
    "\n",
    "# --- √âtape 1 : traitement des entreprises normales ---\n",
    "for company in sorted(os.listdir(root_dir)):\n",
    "    company_folder = os.path.join(root_dir, company)\n",
    "    if not os.path.isdir(company_folder):\n",
    "        continue\n",
    "    print(f\"\\nAnalyse de l‚Äôentreprise : {company}\")\n",
    "\n",
    "    merged_data = {}\n",
    "\n",
    "    # Try to merge extracted sections\n",
    "    if os.path.isdir(company_folder):\n",
    "        for filename in os.listdir(company_folder):\n",
    "            # Nettoyer les caract√®res invisibles et espaces multiples\n",
    "            clean_filename = (\n",
    "                filename.replace(\"\\xa0\", \" \")  # supprime les espaces ins√©cables\n",
    "                .replace(\"\\t\", \" \")            # supprime les tabulations\n",
    "                .strip()\n",
    "            )\n",
    "            clean_filename = \" \".join(clean_filename.split())  # r√©duit les doubles espaces\n",
    "        \n",
    "            # Si le nom nettoy√© diff√®re, on renomme le fichier pour corriger sur disque\n",
    "            if clean_filename != filename:\n",
    "                old_path = os.path.join(company_folder, filename)\n",
    "                new_path = os.path.join(company_folder, clean_filename)\n",
    "                os.rename(old_path, new_path)\n",
    "                filename = clean_filename  # mise √† jour du nom\n",
    "        \n",
    "            if filename.endswith(\".json\"):\n",
    "                # Normaliser pour comparaison\n",
    "                name = (\n",
    "                    os.path.splitext(filename)[0]\n",
    "                    .lower()\n",
    "                    .replace(\".\", \"\")\n",
    "                    .replace(\"\\xa0\", \" \")\n",
    "                    .strip()\n",
    "                )\n",
    "                name = \" \".join(name.split())\n",
    "        \n",
    "                # S√©lection stricte\n",
    "                if any(name == item or name.startswith(item + \" \") for item in selected_items):\n",
    "                    if not any(name == bad or name.startswith(bad + \" \") for bad in not_selected_items):\n",
    "                        file_path = os.path.join(company_folder, filename)\n",
    "                        try:\n",
    "                            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                                data = json.load(f)\n",
    "                            merged_data[filename] = data\n",
    "                        except FileNotFoundError:\n",
    "                            print(f\"‚ö†Ô∏è Fichier introuvable m√™me apr√®s nettoyage : {file_path}\")\n",
    "\n",
    "\n",
    "    # --- Fallback to full 10-K if any selected item is missing ---\n",
    "    if len(merged_data) < len(selected_items):\n",
    "        filing_path = os.path.join(fillings_dir, company_name)\n",
    "        if os.path.isdir(filing_path):\n",
    "            for file in os.listdir(filing_path):\n",
    "                if file.endswith(\".html\") or file.endswith(\".txt\"):\n",
    "                    with open(os.path.join(filing_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    merged_data = {\"full_10k\": content}\n",
    "                    break\n",
    "\n",
    "    # --- Prepare merged text for Claude ---\n",
    "    merged_text = \"\\n\\n\".join(\n",
    "        f\"--- {section} ---\\n{json.dumps(content, ensure_ascii=False)}\"\n",
    "        for section, content in merged_data.items()\n",
    "    )\n",
    "\n",
    "    chunks = chunk_text(merged_text)\n",
    "\n",
    "    # Si plus de 10 chunks ‚Üí on le traitera plus tard\n",
    "    if len(chunks) > 10:\n",
    "        print(f\"Trop volumineux ({len(chunks)} blocs), ajout√© √† la file d‚Äôattente.\")\n",
    "        pending_companies.append(company)\n",
    "        continue\n",
    "\n",
    "    # Traitement imm√©diat\n",
    "    partial_results = []\n",
    "    print(f\"Total blocs {len(chunks)}: {len(merged_text)}\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        prompt_chunk = f\"{base_prompt}\\n\\nSource text (part {i}/{len(chunks)}):\\n{chunk}\"\n",
    "        try:\n",
    "            json_text = call_claude(prompt_chunk).strip()\n",
    "            match = re.search(r\"\\{.*\\}\", json_text, re.DOTALL)\n",
    "            if match:\n",
    "                parsed = json.loads(match.group(0))\n",
    "                partial_results.append(parsed)\n",
    "                print(f\"Chunk {i} trait√© avec succ√®s.\")\n",
    "            else:\n",
    "                print(f\"Chunk {i}: pas de JSON d√©tect√©.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur sur chunk {i}: {e}\")\n",
    "\n",
    "    if not partial_results:\n",
    "        print(f\"Aucun r√©sultat pour {company}\")\n",
    "        continue\n",
    "\n",
    "    final_json = merge_partial_jsons(partial_results)\n",
    "\n",
    "    output_path = os.path.join(output_root, company, f\"{company}.json\")\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# --- √âtape 2 : traitement diff√©r√© des gros fichiers ---\n",
    "if pending_companies:\n",
    "    print(\"\\nTraitement diff√©r√© des entreprises volumineuses :\")\n",
    "    print(\", \".join(pending_companies))\n",
    "    # Tu peux ici relancer le m√™me pipeline, ou le r√©partir sur plusieurs workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6326f83c-3f40-43ef-82f1-da4eb6a4788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "# --- Configuration AWS Bedrock ---\n",
    "config = Config(region_name=\"us-west-2\", connect_timeout=30, read_timeout=300)\n",
    "bedrock = boto3.client(\"bedrock-runtime\", config=config)\n",
    "\n",
    "# --- R√©pertoires racine ---\n",
    "root_dir = \"/home/sagemaker-user/shared/company_profile/extracted_sections\"\n",
    "output_root = \"/home/sagemaker-user/shared/company_profile/profiles\"\n",
    "fillings_dir = \"/home/sagemaker-user/shared/fillings\"\n",
    "\n",
    "# --- S√©lection et exclusion des sections ---\n",
    "selected_items = [\"item 1\", \"item 1a\", \"item 1c\", \"item 2\", \"item 3\", \"item 7\", \"item 7a\", \"item 8\"]\n",
    "not_selected_items = [\"item 1b\", \"item 10\", \"item 11\", \"item 12\", \"item 13\", \"item 14\", \"item 15\", \"item 16\"]\n",
    "\n",
    "# --- Fonction : d√©coupe un texte long ---\n",
    "def chunk_text(text, max_length=200000):\n",
    "    return [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "# --- Fonction : appel Claude ---\n",
    "def call_claude(prompt_text):\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2500,\n",
    "        \"temperature\": 0.1,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt_text}]}],\n",
    "    }\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        body=json.dumps(body),\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    return response_body[\"content\"][0][\"text\"]\n",
    "\n",
    "# --- Fusion intelligente de JSONs partiels ---\n",
    "def merge_partial_jsons(partials):\n",
    "    final_json = {}\n",
    "    for partial in partials:\n",
    "        for key, value in partial.items():\n",
    "            if key in final_json:\n",
    "                existing = final_json.get(key)\n",
    "\n",
    "                # üß© Fusion des listes\n",
    "                if isinstance(value, list):\n",
    "                    if not isinstance(existing, list):\n",
    "                        existing = [] if existing is None else [existing]\n",
    "                    # Si ce sont des dictionnaires, on d√©doublonne proprement\n",
    "                    if all(isinstance(v, dict) for v in value):\n",
    "                        seen = set()\n",
    "                        merged = []\n",
    "                        for item in existing + value:\n",
    "                            ident = json.dumps(item, sort_keys=True)\n",
    "                            if ident not in seen:\n",
    "                                seen.add(ident)\n",
    "                                merged.append(item)\n",
    "                        final_json[key] = merged\n",
    "                    else:\n",
    "                        final_json[key] = list(set(existing + value))\n",
    "\n",
    "                # üßÆ Fusion d‚Äôobjets num√©riques (dicts contenant \"value\")\n",
    "                elif isinstance(value, dict) and \"value\" in value:\n",
    "                    if not isinstance(existing, dict):\n",
    "                        existing = {}\n",
    "                    if \"value\" not in existing or not existing.get(\"value\"):\n",
    "                        existing[\"value\"] = value.get(\"value\")\n",
    "                    if \"unit\" not in existing or not existing.get(\"unit\"):\n",
    "                        existing[\"unit\"] = value.get(\"unit\")\n",
    "                    final_json[key] = existing\n",
    "\n",
    "                # üßæ Fusion des autres types simples\n",
    "                elif existing in (None, \"\", []):\n",
    "                    final_json[key] = value\n",
    "\n",
    "                # üß© Si les deux sont des cha√Ænes, on peut les concat√©ner\n",
    "                elif isinstance(existing, str) and isinstance(value, str):\n",
    "                    if value not in existing:\n",
    "                        final_json[key] = existing + \" \" + value\n",
    "            else:\n",
    "                final_json[key] = value\n",
    "    return final_json\n",
    "\n",
    "# --- Prompt principal ---\n",
    "base_prompt = \"\"\"\n",
    "You are an expert financial analyst specializing in 10-K report analysis.\n",
    "\n",
    "I want you to focus on several points:\n",
    "Give the name of companies mentionned in the text and their relationship with the company of the report.\n",
    "Where are the operations of the company ? What can you say about the ESG and R&D policies ?\n",
    "\n",
    "From the following text, create a **concise JSON** and try to complete the most keys you can.\n",
    "Include measurement units for quantitative data and return **nothing else but the JSON**.\n",
    "\n",
    "For all quantitative data, keep the original number formatting, including commas (e.g., ‚Äú3,450‚Äù instead of ‚Äú3450‚Äù).\n",
    "\n",
    "Expected structure:\n",
    "{\n",
    "  \"date\": \"...\",\n",
    "  \"name\": \"...\",\n",
    "  \"industry\": \"...\",\n",
    "  \"sub_industry\": \"...\",\n",
    "  \"customer_segmentation\": [\"...\"],\n",
    "  \"products\": [\"...\"],\n",
    "  \"supplier_countries\": [\"...\"],\n",
    "  \"supply_chain\": \"...\",\n",
    "  \"geographic_market_segment\": [\"...\"],\n",
    "  \"related_companies\": [{\"company_name\": \"...\", \"relationship_type\": \"...\"}],\n",
    "  \"competitors\": [\"...\"],\n",
    "  \"substitute_products\": [\"...\"],\n",
    "  \"revenue\": {\"value\": null, \"unit\": \"...\", \"variation\": null},\n",
    "  \"net_income\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"gross_margin\": {\"value\": null, \"unit\": \"%\"},\n",
    "  \"income_tax_expense\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"share_buybacks\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"dividends\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"debt\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"interest_expense\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"depreciation\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"free_cash_flow\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"total_assets\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"shareholders_equity\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"ongoing_litigation\": [\"...\"],\n",
    "  \"research_development_expense\": {\"value\": null, \"unit\": \"...\"},\n",
    "  \"research_development_policy\": [\"...\"],\n",
    "  \"ESG_policy\": [\"...\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# --- Fonction pour traiter une entreprise ---\n",
    "def process_company(company, max_block):\n",
    "    company_folder = os.path.join(root_dir, company)\n",
    "    if not os.path.isdir(company_folder):\n",
    "        return None\n",
    "\n",
    "    # print(f\"\\nAnalyse de l‚Äôentreprise : {company}\")\n",
    "    merged_data = {}\n",
    "\n",
    "    # Try to merge extracted sections\n",
    "    if os.path.isdir(company_folder):\n",
    "        for filename in os.listdir(company_folder):\n",
    "            if not filename.lower().endswith(\".json\"):\n",
    "                continue\n",
    "    \n",
    "            # Nettoyage de base\n",
    "            clean_filename = filename.replace(\"\\xa0\", \" \").replace(\"\\t\", \" \").strip()\n",
    "            clean_filename = \" \".join(clean_filename.split())\n",
    "            if clean_filename != filename:\n",
    "                os.rename(os.path.join(company_folder, filename), os.path.join(company_folder, clean_filename))\n",
    "                filename = clean_filename\n",
    "    \n",
    "            # Extraction du \"item X\" ou \"item XA\"\n",
    "            name = os.path.splitext(filename)[0].lower().strip()\n",
    "            match = re.match(r'(item\\s\\d+[a-z]?)', name)  # capture \"item 1\", \"item 1a\", etc.\n",
    "            if not match:\n",
    "                continue\n",
    "    \n",
    "            main_item = match.group(1)  # ex: \"item 1a\"\n",
    "    \n",
    "            # V√©rifie inclusion/exclusion\n",
    "            if main_item in selected_items and main_item not in not_selected_items:\n",
    "                file_path = os.path.join(company_folder, filename)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)\n",
    "                    merged_data[filename] = data\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"‚ö†Ô∏è Fichier introuvable m√™me apr√®s nettoyage : {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    # --- Fallback to full 10-K if any selected item is missing ---\n",
    "    if abs(len(merged_data) - len(selected_items)) > 3:\n",
    "        filing_path = os.path.join(fillings_dir, company)\n",
    "        if os.path.isdir(filing_path):\n",
    "            for file in os.listdir(filing_path):\n",
    "                if file.endswith(\".html\") or file.endswith(\".txt\"):\n",
    "                    with open(os.path.join(filing_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    merged_data = {\"full_10k\": content}\n",
    "                    break\n",
    "\n",
    "    merged_text = \"\\n\\n\".join(\n",
    "        f\"--- {section} ---\\n{json.dumps(content, ensure_ascii=False)}\"\n",
    "        for section, content in merged_data.items()\n",
    "    )\n",
    "\n",
    "    chunks = chunk_text(merged_text)\n",
    "\n",
    "    if len(chunks) > max_block:\n",
    "        print(f\"Trop volumineux {company}, ajout√© √† la file d‚Äôattente.\")\n",
    "        return company  # mettre dans pending_companies\n",
    "\n",
    "    partial_results = []\n",
    "    print(f\"Total blocs {company} {len(chunks)}: {len(merged_text)}\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        prompt_chunk = f\"{base_prompt}\\n\\nSource text (part {i}/{len(chunks)}):\\n{chunk}\"\n",
    "        try:\n",
    "            json_text = call_claude(prompt_chunk).strip()\n",
    "            match = re.search(r\"\\{.*\\}\", json_text, re.DOTALL)\n",
    "            if match:\n",
    "                parsed = json.loads(match.group(0))\n",
    "                partial_results.append(parsed)\n",
    "                #print(f\"Chunk {i} trait√© avec succ√®s.\")\n",
    "            else:\n",
    "                print(f\"Chunk {i}: pas de JSON d√©tect√© {company}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"{company} Erreur sur chunk {i}: {e}\")\n",
    "\n",
    "        # ‚úÖ Limiter √† 1 requ√™te par seconde\n",
    "        time.sleep(1.1)\n",
    "\n",
    "    if not partial_results:\n",
    "        print(f\"Aucun r√©sultat pour {company}\")\n",
    "        return None\n",
    "\n",
    "    final_json = merge_partial_jsons(partial_results)\n",
    "\n",
    "    output_path = os.path.join(output_root, company, f\"{company}.json\")\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_json, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Entreprise {company} fini\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d7836-5123-4e5e-9e3f-a1994cdcd3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Multi-process ---\n",
    "if __name__ == \"__main__\":\n",
    "    companies = sorted(os.listdir(root_dir))\n",
    "    pending_companies = []\n",
    "\n",
    "    with Pool(processes=10) as pool:  # Ajuste le nombre de processes selon ton instance\n",
    "        results = pool.starmap(process_company, [(c, max_block) for c in companies])\n",
    "\n",
    "    # R√©cup√©rer les entreprises volumineuses\n",
    "    pending_companies = [c for c in results if c is not None]\n",
    "\n",
    "    if pending_companies:\n",
    "        print(\"\\nTraitement diff√©r√© des entreprises volumineuses :\")\n",
    "        print(\", \".join(pending_companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f63e6-c8dc-43a2-b6de-3940b06788f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "# Exemple : max_block r√©duit pour retraitement des volumineuses\n",
    "max_block = 100  # tu peux ajuster selon ce que tu veux autoriser\n",
    "\n",
    "if pending_companies:\n",
    "    print(f\"Relance du traitement pour {len(pending_companies)} entreprises volumineuses...\")\n",
    "\n",
    "    # pool.map ne passe qu'un seul argument, on utilise starmap pour passer max_block\n",
    "    with Pool(processes=5) as pool:\n",
    "        results = pool.starmap(process_company, [(c, max_block) for c in pending_companies])\n",
    "\n",
    "    # V√©rifier s'il reste encore des entreprises volumineuses\n",
    "    still_pending = [c for c in results if c is not None]\n",
    "\n",
    "    if still_pending:\n",
    "        print(\"\\nCertaines entreprises restent volumineuses apr√®s cette relance :\")\n",
    "        print(\", \".join(still_pending))\n",
    "    else:\n",
    "        print(\"Toutes les entreprises volumineuses ont √©t√© trait√©es avec succ√®s !\")\n",
    "else:\n",
    "    print(\"Aucune entreprise volumineuse √† retraiter.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13996885-08b7-4d98-8e88-4a6b32e1fd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parmi les 50 entreprises les plus importantes :\n",
      "- 50 entreprises v√©rifi√©es\n",
      "- 6 dossiers ou JSON manquants\n",
      "Dossiers/JSON manquants : ['BRK,B', 'HD', 'GE', 'TMUS', 'MS', 'AXP']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- Param√®tres ---\n",
    "csv_path = \"/home/sagemaker-user/shared/2025-08-15_composition_sp500.csv\"  # ton CSV S&P500\n",
    "root_dir = \"/home/sagemaker-user/shared/company_profile/profiles\"\n",
    "top_n = 50\n",
    "\n",
    "# --- Lire le CSV ---\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# --- Extraire les 100 premiers symboles selon weight d√©croissant ---\n",
    "df_sorted = df.sort_values(\"Weight\", ascending=False)\n",
    "top_symbols = df_sorted.head(top_n)[\"Symbol\"].tolist()\n",
    "\n",
    "# --- V√©rifier l‚Äôexistence des dossiers et pr√©sence d‚Äôun fichier CSV ---\n",
    "missing_companies = []\n",
    "\n",
    "for symbol in top_symbols:\n",
    "    company_folder = os.path.join(root_dir, symbol)\n",
    "    if not os.path.isdir(company_folder):\n",
    "        missing_companies.append(symbol)\n",
    "        continue\n",
    "\n",
    "    # V√©rifie s'il y a au moins un fichier .csv dans le dossier\n",
    "    has_json = any(f.lower().endswith(\".json\") for f in os.listdir(company_folder))\n",
    "    if not has_json:\n",
    "        missing_companies.append(symbol)\n",
    "\n",
    "print(f\"Parmi les {top_n} entreprises les plus importantes :\")\n",
    "print(f\"- {len(top_symbols)} entreprises v√©rifi√©es\")\n",
    "print(f\"- {len(missing_companies)} dossiers ou JSON manquants\")\n",
    "print(\"Dossiers/JSON manquants :\", missing_companies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ed96454-0536-4650-9a0a-07143e6e0015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total entreprises sans JSON : 77\n",
      "['.ipynb_checkpoints', 'ALL', 'AME', 'AMP', 'APA', 'ARE', 'AXP', 'BA', 'C', 'CAH', 'CARR', 'CCI', 'CHD', 'CI', 'COIN', 'CPRT', 'DAL', 'DD', 'DECK', 'DGX', 'DHR', 'DLR', 'DOW', 'DUK', 'EBAY', 'EIX', 'ELV', 'EMN', 'EQIX', 'EQR', 'EXPD', 'EXPE', 'FCX', 'FDS', 'FDX', 'FI', 'GD', 'GE', 'HBAN', 'HD', 'HON', 'HST', 'HWM', 'IDXX', 'INTC', 'IP', 'KIM', 'LEN', 'LUV', 'MET', 'MS', 'NI', 'O', 'OTIS', 'PLD', 'PODD', 'PRU', 'RL', 'RMD', 'SBAC', 'SCHW', 'SNA', 'SPGI', 'STZ', 'SWK', 'SYF', 'TFC', 'TJX', 'TMUS', 'TPL', 'TT', 'TXT', 'TYL', 'VLTO', 'WDC', 'WST', 'WY']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_folder = \"/home/sagemaker-user/shared/company_profile/profiles\"\n",
    "missing_json = []\n",
    "\n",
    "# Parcourt chaque entreprise dans le dossier\n",
    "for company in sorted(os.listdir(input_folder)):\n",
    "    company_path = os.path.join(input_folder, company)\n",
    "    if not os.path.isdir(company_path):\n",
    "        continue\n",
    "\n",
    "    # V√©rifie s'il y a au moins un fichier JSON\n",
    "    has_json = any(f.lower().endswith(\".json\") for f in os.listdir(company_path))\n",
    "\n",
    "    if not has_json:\n",
    "        missing_json.append(company)\n",
    "\n",
    "# R√©sum√©\n",
    "print(f\"\\nTotal entreprises sans JSON : {len(missing_json)}\")\n",
    "print(missing_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3116f5-a04d-420f-b281-9d12c0bac7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# --- Configuration Bedrock / r√©gion ---\n",
    "config = Config(region_name=\"us-west-2\", connect_timeout=30, read_timeout=300)\n",
    "bedrock = boto3.client(\"bedrock-runtime\", config=config)\n",
    "\n",
    "# --- Liste des symboles √† g√©n√©rer (remplace/compl√®te si besoin) ---\n",
    "symbols = [\n",
    "    \"ALL\",\"AME\",\"AMP\",\"APA\",\"ARE\",\"AXP\",\"BA\",\"C\",\"CAH\",\"CARR\",\"CCI\",\"CHD\",\"CI\",\"COIN\",\"CPRT\",\n",
    "    \"DAL\",\"DD\",\"DECK\",\"DGX\",\"DHR\",\"DLR\",\"DOW\",\"DUK\",\"EBAY\",\"EIX\",\"ELV\",\"EMN\",\"EQIX\",\"EQR\",\n",
    "    \"EXPD\",\"EXPE\",\"FCX\",\"FDS\",\"FDX\",\"FI\",\"GD\",\"GE\",\"HBAN\",\"HD\",\"HON\",\"HST\",\"HWM\",\"IDXX\",\n",
    "    \"INTC\",\"IP\",\"KIM\",\"LEN\",\"LUV\",\"MET\",\"MS\",\"NI\",\"O\",\"OTIS\",\"PLD\",\"PODD\",\"PRU\",\"RL\",\"RMD\",\n",
    "    \"SBAC\",\"SCHW\",\"SNA\",\"SPGI\",\"STZ\",\"SWK\",\"SYF\",\"TFC\",\"TJX\",\"TMUS\",\"TPL\",\"TT\",\"TXT\",\"TYL\",\n",
    "    \"VLTO\",\"WDC\",\"WST\",\"WY\"\n",
    "]\n",
    "\n",
    "# --- Dossiers ---\n",
    "profiles_root = \"/home/sagemaker-user/shared/company_profile/profiles\"\n",
    "\n",
    "# --- Prompt template (FR) ---\n",
    "prompt_template = \"\"\"\n",
    "Tu es un analyste financier expert. √Ä partir de ta connaissance publique (sans acc√®s √† un 10-K local), remplis STRICTEMENT,\n",
    "et ne retourne RIEN d'autre qu'un objet JSON, au format exact suivant (respecte les cl√©s et types):\n",
    "\n",
    "{{\n",
    "  \"date\": \"...\",\n",
    "  \"name\": \"...\",\n",
    "  \"industry\": \"...\",\n",
    "  \"sub_industry\": \"...\",\n",
    "  \"customer_segmentation\": [\"...\"],\n",
    "  \"products\": [\"...\"],\n",
    "  \"supplier_countries\": [\"...\"],\n",
    "  \"supply_chain\": \"...\",\n",
    "  \"geographic_market_segment\": [\"...\"],\n",
    "  \"related_companies\": [{{\"company_name\": \"...\", \"relationship_type\": \"...\"}}],\n",
    "  \"competitors\": [\"...\"],\n",
    "  \"substitute_products\": [\"...\"],\n",
    "  \"revenue\": {{\"value\": null, \"unit\": \"...\", \"variation\": null}},\n",
    "  \"net_income\": {{\"value\": null, \"unit\": \"...\"}},\n",
    "  \"gross_margin\": {{\"value\": null, \"unit\": \"%\"}},\n",
    "  \"income_tax_expense\": {{\"value\": null, \"unit\": \"...\"}},\n",
    "  \"share_buybacks\": {{\"value\": null, \"unit\": \"...\"}},\n",
    "  \"dividends\": {{\"value\": null, \"unit\": \"...\"}},\n",
    "  \"debt\": {{\"value\": null, \"unit\": \"...\"}},\n",
    "  \"interest_expense\": {{\"value\": null, \"unit\": \"...\"}},\n",
    "  \"depreciation\": {{\"value\": null, \"unit\": \"...\"}},\n",
    "  \"free_cash_flow\": {{\"value\": null, \"unit\": \"...\"}},\n",
    "  \"total_assets\": {{\"value\": null, \"unit\": \"...\"}},\n",
    "  \"shareholders_equity\": {{\"value\": null, \"unit\": \"...\"}},\n",
    "  \"ongoing_litigation\": [\"...\"],\n",
    "  \"research_development_expense\": {{\"value\": null, \"unit\": \"...\"}},\n",
    "  \"research_development_policy\": [\"...\"],\n",
    "  \"ESG_policy\": [\"...\"]\n",
    "}}\n",
    "\n",
    "Remplis les champs avec les valeurs les plus plausibles √† partir de ta connaissance publique pour l'entreprise dont le symbole boursier est: \"{symbol}\".\n",
    "Si tu n'as pas d'information pour un champ, laisse `\"value\": null` ou `[]` ou `\"\"` selon le type.\n",
    "Retourne uniquement le JSON (aucune explication).\n",
    "\"\"\"\n",
    "\n",
    "# --- Fonction d'appel Bedrock / Claude ---\n",
    "def call_claude(prompt_text, model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\"):\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2500,\n",
    "        \"temperature\": 0.0,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt_text}]}],\n",
    "    }\n",
    "    resp = bedrock.invoke_model(modelId=model_id, body=json.dumps(body))\n",
    "    resp_body = json.loads(resp[\"body\"].read())\n",
    "    # The model text often at [\"content\"][0][\"text\"]\n",
    "    return resp_body[\"content\"][0][\"text\"]\n",
    "\n",
    "def extract_first_json(s: str):\n",
    "    \"\"\"\n",
    "    Extrait le premier objet JSON complet trouv√© dans une cha√Æne.\n",
    "    Ignore le reste du texte et g√®re les accolades imbriqu√©es.\n",
    "    \"\"\"\n",
    "    start = s.find('{')\n",
    "    if start == -1:\n",
    "        return None\n",
    "\n",
    "    stack = 0\n",
    "    for i in range(start, len(s)):\n",
    "        if s[i] == '{':\n",
    "            stack += 1\n",
    "        elif s[i] == '}':\n",
    "            stack -= 1\n",
    "            if stack == 0:\n",
    "                return s[start:i+1]\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Boucle principale ---\n",
    "os.makedirs(profiles_root, exist_ok=True)\n",
    "errors = []\n",
    "generated = 0\n",
    "\n",
    "for sym in symbols:\n",
    "    out_dir = os.path.join(profiles_root, sym)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, f\"{sym}.json\")\n",
    "\n",
    "    # Skip si d√©j√† pr√©sent\n",
    "    if os.path.exists(out_path):\n",
    "        print(\"D√©j√† existant, saut.\")\n",
    "        continue\n",
    "\n",
    "    prompt = prompt_template.format(symbol=sym)\n",
    "\n",
    "    try:\n",
    "        raw = call_claude(prompt)\n",
    "        json_text = extract_first_json(raw)\n",
    "\n",
    "        if not json_text:\n",
    "            print(f\"Aucun JSON trouv√© pour {sym}\")\n",
    "            errors.append((sym, \"no_json_in_response\"))\n",
    "            continue\n",
    "        # Validation et sauvegarde\n",
    "        parsed = json.loads(json_text)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(parsed, f, ensure_ascii=False, indent=2)\n",
    "        generated += 1\n",
    "        print(f\"{sym} sauvegard√© avec succ√®s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur pour {sym}: {e}\")\n",
    "        errors.append((sym, str(e)))\n",
    "\n",
    "    time.sleep(1.1)\n",
    "\n",
    "# --- R√©sum√© ---\n",
    "print(f\"Generated profiles: {generated}\")\n",
    "print(f\"Errors: {len(errors)}\")\n",
    "if errors:\n",
    "    for e in errors[:50]:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6b3a8-3226-4daa-97d9-f886d1ab0571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
